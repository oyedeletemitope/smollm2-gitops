apiVersion: apps/v1
kind: Deployment
metadata:
  name: smollm2-deployment
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: smollm2-vllm-vllm
  template:
    metadata:
      labels:
        app: smollm2-vllm-vllm
    spec:
      containers:
        - name: vllm
          image: "jozu.ml/oyedeletemitope76/smollm2-vllm/vllm-cpu:v1.0.0"
          command: ["vllm", "serve", "TinyLlama/TinyLlama-1.1B-Chat-v1.0"] 
          ports:
            - containerPort: 8000
          env:
            - name: VLLM_DEVICE
              value: "cpu"
            - name: VLLM_LOGGING_LEVEL
              value: "INFO"
            - name: CUDA_VISIBLE_DEVICES
              value: ""
            - name: VLLM_WORKER_MULTIPROC_METHOD
              value: "spawn"
            - name: VLLM_ENFORCE_EAGER
              value: "true"
            - name: VLLM_DISABLE_CUSTOM_ALL_REDUCE
              value: "true"
            - name: VLLM_CPU_SWAP_SPACE
              value: "0"
          resources:
            limits:
              memory: 2Gi
              cpu: "2"
            requests:
              memory: 1Gi
              cpu: "1"
      nodeSelector:
        kubernetes.io/arch: arm64
